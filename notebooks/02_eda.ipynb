{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis on Reddit posts data.\n",
        "\n",
        "## Objectives\n",
        "1. Understand data structure and types\n",
        "2. Identify missing values and data quality issues\n",
        "3. Analyze distributions of key variables\n",
        "4. Explore relationships between variables\n",
        "5. Generate insights and visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = Path(\"../data/processed/ingested_data.csv\")\n",
        "\n",
        "if data_path.exists():\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
        "else:\n",
        "    # Fallback to output directory\n",
        "    output_path = Path(\"../data/output\")\n",
        "    csv_files = list(output_path.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
        "        df = pd.read_csv(latest_file)\n",
        "        print(f\"Loaded {len(df)} rows from {latest_file.name}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No data files found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information\n",
        "print(\"=\" * 50)\n",
        "print(\"DATA OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Missing Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing Count': df.isnull().sum(),\n",
        "    'Missing Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(\"Missing Values Analysis:\")\n",
        "print(missing_data[missing_data['Missing Count'] > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "if missing_data['Missing Count'].sum() > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    missing_data[missing_data['Missing Count'] > 0].plot(\n",
        "        x='Column', y='Missing Percentage', kind='barh', figsize=(10, 6)\n",
        "    )\n",
        "    plt.title('Missing Values by Column')\n",
        "    plt.xlabel('Missing Percentage (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Subreddit Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subreddit distribution\n",
        "if 'subreddit' in df.columns:\n",
        "    subreddit_counts = df['subreddit'].value_counts()\n",
        "    \n",
        "    print(\"Subreddit Distribution:\")\n",
        "    print(subreddit_counts)\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Bar chart\n",
        "    subreddit_counts.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "    axes[0].set_title('Posts by Subreddit')\n",
        "    axes[0].set_xlabel('Subreddit')\n",
        "    axes[0].set_ylabel('Number of Posts')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Pie chart\n",
        "    subreddit_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
        "    axes[1].set_title('Subreddit Distribution')\n",
        "    axes[1].set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Score and Engagement Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score distribution\n",
        "if 'score' in df.columns:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Score distribution\n",
        "    df['score'].hist(bins=50, ax=axes[0, 0], color='skyblue', edgecolor='black')\n",
        "    axes[0, 0].set_title('Score Distribution')\n",
        "    axes[0, 0].set_xlabel('Score')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # Log scale\n",
        "    df['score'].apply(lambda x: np.log1p(x) if x >= 0 else 0).hist(bins=50, ax=axes[0, 1], color='lightgreen', edgecolor='black')\n",
        "    axes[0, 1].set_title('Score Distribution (Log Scale)')\n",
        "    axes[0, 1].set_xlabel('Log(Score + 1)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    # Comments distribution\n",
        "    if 'num_comments' in df.columns:\n",
        "        df['num_comments'].hist(bins=50, ax=axes[1, 0], color='coral', edgecolor='black')\n",
        "        axes[1, 0].set_title('Number of Comments Distribution')\n",
        "        axes[1, 0].set_xlabel('Number of Comments')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "        \n",
        "        # Score vs Comments\n",
        "        axes[1, 1].scatter(df['num_comments'], df['score'], alpha=0.5, s=10)\n",
        "        axes[1, 1].set_title('Score vs Number of Comments')\n",
        "        axes[1, 1].set_xlabel('Number of Comments')\n",
        "        axes[1, 1].set_ylabel('Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\nScore Statistics:\")\n",
        "    print(df['score'].describe())\n",
        "    if 'num_comments' in df.columns:\n",
        "        print(\"\\nComments Statistics:\")\n",
        "        print(df['num_comments'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert created_utc to datetime if needed\n",
        "if 'created_utc' in df.columns:\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df['created_utc']):\n",
        "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', errors='coerce')\n",
        "    \n",
        "    df['created_date'] = df['created_utc'].dt.date\n",
        "    df['created_hour'] = df['created_utc'].dt.hour\n",
        "    df['created_day_of_week'] = df['created_utc'].dt.day_name()\n",
        "    \n",
        "    # Temporal visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Posts over time\n",
        "    daily_counts = df.groupby('created_date').size()\n",
        "    daily_counts.plot(ax=axes[0, 0], color='steelblue')\n",
        "    axes[0, 0].set_title('Posts Over Time')\n",
        "    axes[0, 0].set_xlabel('Date')\n",
        "    axes[0, 0].set_ylabel('Number of Posts')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Posts by hour\n",
        "    hourly_counts = df['created_hour'].value_counts().sort_index()\n",
        "    hourly_counts.plot(kind='bar', ax=axes[0, 1], color='lightcoral')\n",
        "    axes[0, 1].set_title('Posts by Hour of Day')\n",
        "    axes[0, 1].set_xlabel('Hour')\n",
        "    axes[0, 1].set_ylabel('Number of Posts')\n",
        "    \n",
        "    # Posts by day of week\n",
        "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    day_counts = df['created_day_of_week'].value_counts().reindex(day_order)\n",
        "    day_counts.plot(kind='bar', ax=axes[1, 0], color='mediumseagreen')\n",
        "    axes[1, 0].set_title('Posts by Day of Week')\n",
        "    axes[1, 0].set_xlabel('Day of Week')\n",
        "    axes[1, 0].set_ylabel('Number of Posts')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Average score by hour\n",
        "    if 'score' in df.columns:\n",
        "        avg_score_by_hour = df.groupby('created_hour')['score'].mean()\n",
        "        avg_score_by_hour.plot(ax=axes[1, 1], color='gold', marker='o')\n",
        "        axes[1, 1].set_title('Average Score by Hour')\n",
        "        axes[1, 1].set_xlabel('Hour')\n",
        "        axes[1, 1].set_ylabel('Average Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length analysis\n",
        "if 'title' in df.columns:\n",
        "    df['title_length'] = df['title'].str.len()\n",
        "    df['title_word_count'] = df['title'].str.split().str.len()\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    df['title_length'].hist(bins=50, ax=axes[0], color='plum', edgecolor='black')\n",
        "    axes[0].set_title('Title Length Distribution')\n",
        "    axes[0].set_xlabel('Character Count')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    \n",
        "    df['title_word_count'].hist(bins=30, ax=axes[1], color='khaki', edgecolor='black')\n",
        "    axes[1].set_title('Title Word Count Distribution')\n",
        "    axes[1].set_xlabel('Word Count')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTitle Statistics:\")\n",
        "    print(f\"Average length: {df['title_length'].mean():.2f} characters\")\n",
        "    print(f\"Average word count: {df['title_word_count'].mean():.2f} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) > 1:\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print strong correlations\n",
        "    print(\"\\nStrong Correlations (|r| > 0.5):\")\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_val = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_val) > 0.5:\n",
        "                print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {corr_val:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Insights Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary insights\n",
        "print(\"=\" * 50)\n",
        "print(\"EDA SUMMARY INSIGHTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n1. Dataset Size: {len(df)} posts, {len(df.columns)} features\")\n",
        "print(f\"\\n2. Date Range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "print(f\"\\n3. Subreddits: {df['subreddit'].nunique()} unique subreddits\")\n",
        "if 'score' in df.columns:\n",
        "    print(f\"\\n4. Score Range: {df['score'].min()} to {df['score'].max()} (mean: {df['score'].mean():.2f})\")\n",
        "if 'num_comments' in df.columns:\n",
        "    print(f\"\\n5. Comments Range: {df['num_comments'].min()} to {df['num_comments'].max()} (mean: {df['num_comments'].mean():.2f})\")\n",
        "print(f\"\\n6. Missing Values: {df.isnull().sum().sum()} total missing values\")\n",
        "print(f\"\\n7. Duplicate IDs: {df['id'].duplicated().sum()} duplicates\")\n",
        "\n",
        "# Save processed data for next steps\n",
        "output_path = Path(\"../data/processed/eda_data.csv\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"\\n8. Processed data saved to: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
