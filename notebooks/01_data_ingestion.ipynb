{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Ingestion Notebook\n",
        "\n",
        "This notebook handles data ingestion from various sources:\n",
        "- Reddit API extraction\n",
        "- Loading from local CSV files\n",
        "- Loading from S3\n",
        "- Data quality checks\n",
        "\n",
        "## Objectives\n",
        "1. Extract data from Reddit API\n",
        "2. Load existing data files\n",
        "3. Perform initial data quality checks\n",
        "4. Save data in standardized format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from src.ingestion.reddit_extractor import RedditExtractor\n",
        "from src.processing.data_validator import DataValidator\n",
        "from src.utils.config import get_config\n",
        "from src.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Extract from Reddit API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Reddit extractor\n",
        "extractor = RedditExtractor()\n",
        "\n",
        "# Define subreddits to extract\n",
        "subreddits = ['science', 'politics', 'technology', 'relationships']\n",
        "\n",
        "# Extract posts\n",
        "df_reddit = extractor.extract_posts_batch(\n",
        "    subreddits=subreddits,\n",
        "    time_filter='all',\n",
        "    limit_per_subreddit=1000,\n",
        "    sort='top'\n",
        ")\n",
        "\n",
        "print(f\"Extracted {len(df_reddit)} posts\")\n",
        "df_reddit.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Load from Local CSV File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load from local CSV file\n",
        "data_path = Path(\"../data/output\")\n",
        "csv_files = list(data_path.glob(\"*.csv\"))\n",
        "\n",
        "if csv_files:\n",
        "    # Load the most recent file\n",
        "    latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
        "    print(f\"Loading: {latest_file}\")\n",
        "    df_local = pd.read_csv(latest_file)\n",
        "    print(f\"Loaded {len(df_local)} rows from {latest_file.name}\")\n",
        "    df_local.head()\n",
        "else:\n",
        "    print(\"No CSV files found in data/output directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 3: Load from S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load from S3 (if configured)\n",
        "try:\n",
        "    import boto3\n",
        "    from src.utils.config import get_config\n",
        "    \n",
        "    config = get_config()\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=config.aws.access_key_id,\n",
        "        aws_secret_access_key=config.aws.secret_access_key,\n",
        "        region_name=config.aws.region\n",
        "    )\n",
        "    \n",
        "    # List objects in S3\n",
        "    bucket = config.aws.bucket_name\n",
        "    prefix = \"raw/reddit/\"\n",
        "    \n",
        "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
        "    \n",
        "    if 'Contents' in response:\n",
        "        # Get the most recent file\n",
        "        objects = sorted(response['Contents'], key=lambda x: x['LastModified'], reverse=True)\n",
        "        latest_key = objects[0]['Key']\n",
        "        \n",
        "        # Download and load\n",
        "        obj = s3_client.get_object(Bucket=bucket, Key=latest_key)\n",
        "        df_s3 = pd.read_csv(obj['Body'])\n",
        "        print(f\"Loaded {len(df_s3)} rows from S3: {latest_key}\")\n",
        "        df_s3.head()\n",
        "    else:\n",
        "        print(\"No files found in S3\")\n",
        "except Exception as e:\n",
        "    print(f\"S3 loading not available: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the loaded dataframe (choose df_reddit, df_local, or df_s3)\n",
        "df = df_reddit if 'df_reddit' in locals() and not df_reddit.empty else df_local if 'df_local' in locals() else None\n",
        "\n",
        "if df is not None:\n",
        "    validator = DataValidator()\n",
        "    validation_result = validator.validate(df)\n",
        "    \n",
        "    print(validation_result)\n",
        "    print(\"\\nValidation Statistics:\")\n",
        "    for key, value in validation_result.stats.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "else:\n",
        "    print(\"No data loaded. Please run one of the ingestion options above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save data for next steps\n",
        "if df is not None:\n",
        "    output_path = Path(\"../data/processed/ingested_data.csv\")\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "    print(f\"Shape: {df.shape}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
